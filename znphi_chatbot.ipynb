{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckysiabula-bit/healh_chartbot_research/blob/main/znphi_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QkF7djzSdYEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a folder in your Google Drive\n",
        "project_path = \"/content/drive/MyDrive/Measles_Chatbot_Project\"\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "print(f\"ðŸ“‚ Project folder created at: {project_path}\")"
      ],
      "metadata": {
        "id": "hLFAC2kCdbcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face libraries\n",
        "!pip install -q -U accelerate peft transformers trl\n",
        "!pip install -q datasets\n",
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "id": "MEezKei8desH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Example of IDSR-aligned data (You must replace this with your actual IDSR data)\n",
        "data = [\n",
        "\n",
        "  {\n",
        "    \"instruction\": \"What is the standard case definition for a suspected measles case?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Standard Case Definitions (General Knowledge aligned with Section 11 header).\",\n",
        "    \"response\": \"A suspected measles case is defined as any person with fever and maculopapular (non-vesicular) generalized rash and cough, coryza (runny nose), or conjunctivitis (red eyes), OR any person in whom a clinician suspects measles.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What are the immediate reporting requirements for a suspected measles case?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Section 2: Reporting.\",\n",
        "    \"response\": \"Measles is an epidemic-prone disease requiring immediate reporting. Any suspected case must be reported to the next level within 24 hours using the Case-based Reporting Form. It should also be notified via the fastest means possible (e.g., phone or SMS).\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the specific purpose of the Euroimmun Anti-Measles ELISA test?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 1: Purpose.\",\n",
        "    \"response\": \"The Euroimmun Anti-Measles ELISA test is designed for the semi-quantitative in vitro determination of human IgM class antibodies against the Measles virus nucleoprotein in serum or plasma. It is used to confirm a recent measles infection.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What sample types are accepted for the Measles IgM ELISA test?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 4: Specimen.\",\n",
        "    \"response\": \"The test accepts human serum, or plasma collected using EDTA, heparin, or citrate. Samples should not be lipemic, hemolytic, or icteric.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How should serum samples be stored if testing is not performed immediately?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 4: Specimen.\",\n",
        "    \"response\": \"If testing is performed within 1 week, samples can be stored at 2Â°C to 8Â°C. For longer storage, samples must be frozen at -20Â°C or colder. Frost-free freezers should be avoided to prevent freeze-thaw cycles which degrade antibodies.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What are the critical safety precautions when handling reagents and samples for the Measles ELISA?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 5: Safety.\",\n",
        "    \"response\": \"Staff must wear gloves and PPE. All blood and serum must be handled as infectious. The first three steps of the procedure (sample preparation) must be performed in a biosafety cabinet to prevent aerosol exposure. Surfaces should be disinfected with 1% Virkon or 70% Alcohol.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How do you prepare the wash buffer for the Measles ELISA test?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Dilute the 10X washing buffer concentrate with distilled water at a 1:10 ratio. For example, to prepare enough for one microplate strip, mix 5 ml of 10X wash buffer concentrate with 45 ml of distilled water.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the exact dilution ratio for patient samples in the Measles ELISA test?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Patient samples must be diluted 1:101. This is achieved by mixing 10 Âµl of the patient sample with 1000 Âµl (1.0 ml) of the sample buffer containing RF-Absorbent.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Do the kit controls (positive, negative, calibrator) require dilution?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"No, the kit controls are ready for use and must NOT be diluted.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the incubation protocol for the diluted test samples before adding them to the plate?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"After mixing the sample with the buffer, the diluted test samples must be incubated for 10 minutes at room temperature (18Â°Câ€“25Â°C) before being dispensed into the microtiter wells.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Describe the sample incubation step on the microplate.\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Add 100 Âµl of diluted samples and undiluted controls to the appropriate wells. Incubate the plate for 30 minutes at room temperature (18Â°Câ€“25Â°C).\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the washing procedure after the first incubation?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Wash all wells with 300 Âµl of diluted washing solution (450 Âµl if using an automated washer). Leave the buffer in wells for 30â€“60 seconds, then aspirate or shake out. Repeat this process for a total of 3 washes. Blot the plate on a paper towel to remove remaining liquid.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How long is the conjugate incubation step?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Add 100 Âµl of enzyme conjugate to each well and incubate for 30 minutes at room temperature.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What are the conditions for the substrate incubation?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Add 100 Âµl of chromogen/substrate solution to each well and incubate for 15 minutes at room temperature. It is critical to protect the plate from direct light during this step.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How should the results be read after adding the stop solution?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 7: Procedure.\",\n",
        "    \"response\": \"Read the Optical Density (OD) at 450 nm within 30 minutes of adding the stop solution. A reference wavelength between 620 nm and 650 nm (e.g., 630 nm) should be used.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How are the results of the Measles ELISA interpreted?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 10: Reference Range.\",\n",
        "    \"response\": \"Results are interpreted using a ratio based on the calibrator:\\n- Ratio < 0.8: Negative\\n- Ratio â‰¥ 0.8 to < 1.1: Equivocal (Borderline)\\n- Ratio â‰¥ 1.1: Positive\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What should be done if a sample yields an equivocal result?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 10: Reference Range (Standard interpretation).\",\n",
        "    \"response\": \"If a result is equivocal (ratio between 0.8 and 1.1), the test should be repeated using a fresh sample collected 7 to 10 days later.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the procedure if a suspected measles sample tests negative for measles IgM?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 12: Notes.\",\n",
        "    \"response\": \"If a suspected measles sample tests negative for anti-measles virus IgM antibodies, it must then be tested for anti-rubella virus IgM antibodies.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How should waste generated from the ELISA test be disposed of?\",\n",
        "    \"context\": \"VIR-TECH-011-v2 Detection of Anti-Measles IgM Antibodies, Section 13: Disposal.\",\n",
        "    \"response\": \"All waste must be placed securely in a biohazard bag and autoclaved at 121Â°C for 15 minutes before being sent for incineration.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What does a 'confirmed' measles outbreak look like?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Section 11 (General Thresholds).\",\n",
        "    \"response\": \"A confirmed measles outbreak is typically defined as 3 or more IgM positive measles cases detected in a health facility or district within a single month.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Who should be involved in Community-Based Surveillance (CBS) for measles?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Section 1: Community Surveillance.\",\n",
        "    \"response\": \"Community health workers, traditional healers, birth attendants, community leaders, and volunteers should be involved. They should be trained to recognize 'fever with rash' and report suspected cases immediately to the nearest health facility.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the Alert Threshold for measles?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Section 11 (General Thresholds).\",\n",
        "    \"response\": \"The alert threshold for measles is a single (1) suspected case. Even one case requires immediate investigation to confirm or rule out an outbreak.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"When should Vitamin A be administered to a measles patient?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Measles Treatment Guidelines.\",\n",
        "    \"response\": \"Vitamin A should be administered immediately upon diagnosis of measles. A second dose is given the next day, and a third dose is given 2â€“4 weeks later if the child has clinical signs of Vitamin A deficiency.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the recommended Vitamin A dose for an infant under 6 months?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Measles Treatment Guidelines.\",\n",
        "    \"response\": \"For infants less than 6 months of age, the recommended Vitamin A dose is 50,000 IU.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is the recommended Vitamin A dose for children aged 12 months and older?\",\n",
        "    \"context\": \"IDSR 3rd Edition - Measles Treatment Guidelines.\",\n",
        "    \"response\": \"For children aged 12 months and older, the recommended Vitamin A dose is 200,000 IU.\"\n",
        "  }\n",
        "\n",
        "]\n",
        "\n",
        "# Convert the list of dictionaries to a Hugging Face Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# Define the formatting function to turn the data into a training prompt\n",
        "def format_instruction(sample):\n",
        "    # This template is used by the SFTTrainer to convert the data columns into a single string for training\n",
        "    return f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Context:\n",
        "{sample['context']}\n",
        "\n",
        "### Response:\n",
        "{sample['response']}\"\"\"\n",
        "\n",
        "# Apply the formatting function\n",
        "dataset = dataset.map(lambda x: {'text': format_instruction(x)})"
      ],
      "metadata": {
        "id": "K7ioU8r_dklB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "# 1. Quantization Configuration (QLoRA)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # NormalFloat 4-bit, which is optimal for LLM weights\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float32 # Compute in float32 to avoid BFloat16 issues\n",
        ")\n",
        "\n",
        "# 2. Load the base LLM (Mistral 7B)\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically handles model placement on the GPU\n",
        "    dtype=torch.float32 # Explicitly set model's default data type to float32\n",
        ")\n",
        "model.to('cuda') # Explicitly move model to CUDA\n",
        "# Note: You may need to log in to Hugging Face to access the model\n",
        "# from huggingface_hub import notebook_login; notebook_login()\n",
        "\n",
        "# 3. LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8, # LoRA rank: reduced from 16 to 8 for memory optimization\n",
        "    lora_alpha=32, # Scaling factor for LoRA updates\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target common linear layers in the model\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "al4288Ivdnqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 1. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{project_path}/adapter_weights\", # Directory to save the checkpoint\n",
        "    num_train_epochs=3, # Train for 3 epochs (Adjust based on dataset size)\n",
        "    per_device_train_batch_size=2, # Adjust based on GPU memory\n",
        "    gradient_accumulation_steps=2, # Increase this to simulate a larger batch size if memory is an issue\n",
        "    optim=\"paged_adamw_8bit\", # Memory-efficient AdamW\n",
        "    learning_rate=2e-4, # Standard learning rate for LoRA\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=False, # Use 16-bit floating point precision\n",
        "    bf16=False, # Explicitly disable BFloat16 mixed precision\n",
        "    gradient_checkpointing=False, # Disable gradient checkpointing to avoid potential device issues\n",
        ")\n",
        "\n",
        "# 2. Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# 3. Start Training\n",
        "trainer.train()\n",
        "\n",
        "# 4. Save the Adapter Weights\n",
        "trainer.model.save_pretrained(\"mistral-7b-measles-qlora-adapter\")"
      ],
      "metadata": {
        "id": "i1xpL6THdrb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model (quantized)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter on top of the base model\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"mistral-7b-measles-qlora-adapter\")\n",
        "ft_model = ft_model.merge_and_unload() # Optional: Merge the adapter weights back into the base model weights\n",
        "\n",
        "# Example Inference\n",
        "prompt = \"### Instruction: What is the probable case definition for measles? ### Context: [Your retrieved IDSR text snippets] ### Response:\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
        "outputs = ft_model.generate(**inputs, max_new_tokens=256)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "060AqR-ydvD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "app = FastAPI(title=\"Measles Chatbot API\")\n",
        "\n",
        "# --- Global Variables for Model ---\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "adapter_path = \"./mistral-7b-measles-qlora-adapter\" # Path to your saved fine-tuned adapter\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "# --- Request Data Model ---\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    history: list = [] # Optional: for multi-turn conversation context\n",
        "\n",
        "# --- Startup Event: Load Model ---\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    global model, tokenizer\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "\n",
        "    # 1. Load Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "    # 2. QLoRA Config (Same as training)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # 3. Load Base Model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # 4. Load Fine-Tuned Adapter\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "# --- Helper: Dummy RAG Retriever ---\n",
        "def retrieve_idsr_context(query: str):\n",
        "    \"\"\"\n",
        "    TODO: Replace this with your actual Vector DB search (FAISS/Chroma).\n",
        "    For now, we return a static snippet from your guidelines.\n",
        "    \"\"\"\n",
        "    # This simulates finding relevant sections in the IDSR docs\n",
        "    return \"\"\"\n",
        "    IDSR Guideline Section 11:\n",
        "    A suspected measles case is any person with fever and maculopapular (non-vesicular)\n",
        "    generalized rash and cough, coryza or conjunctivitis (red eyes).\n",
        "    \"\"\"\n",
        "\n",
        "# --- API Endpoint: /chat ---\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    global model, tokenizer\n",
        "\n",
        "    user_query = request.message\n",
        "\n",
        "    # 1. Retrieve Context (RAG Step)\n",
        "    context = retrieve_idsr_context(user_query)\n",
        "\n",
        "    # 2. Format Prompt (MUST match fine-tuning format)\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "{user_query}\n",
        "\n",
        "### Context:\n",
        "{context}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # 3. Tokenize and Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    # 4. Decode Response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the newly generated part (after \"### Response:\")\n",
        "    if \"### Response:\" in full_response:\n",
        "        final_answer = full_response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        final_answer = full_response\n",
        "\n",
        "    return {\"response\": final_answer, \"context_used\": context}\n",
        "\n",
        "# --- Health Check ---\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"active\", \"model_loaded\": model is not None}"
      ],
      "metadata": {
        "id": "Q5JILBgcdy70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "4OuEnE8id123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from threading import Thread\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- 1. Define the FastAPI App ---\n",
        "app = FastAPI(title=\"Measles Chatbot API\")\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    history: list = []\n",
        "\n",
        "# This endpoint uses the 'model' and 'tokenizer' you loaded earlier in the notebook\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    # Check if model is loaded in the notebook\n",
        "    if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "        return {\"response\": \"Error: Model not loaded in memory. Please run the fine-tuning/loading cells first.\"}\n",
        "\n",
        "    user_query = request.message\n",
        "\n",
        "    # Simple formatting (ensure this matches your training format)\n",
        "    prompt = f\"### Instruction:\\n{user_query}\\n\\n### Response:\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer part\n",
        "    if \"### Response:\" in full_response:\n",
        "        final_answer = full_response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        final_answer = full_response\n",
        "\n",
        "    return {\"response\": final_answer}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# --- 2. Start the Server & Tunnel ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_server():\n",
        "    # host=\"127.0.0.1\" prevents the IPv6 error\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
        "\n",
        "# Stop any existing threads/tunnels to prevent conflicts\n",
        "ngrok.kill()\n",
        "\n",
        "# Start server in background\n",
        "thread = Thread(target=run_server)\n",
        "thread.start()\n",
        "\n",
        "# Open the public tunnel\n",
        "# Make sure your token is set! If not, uncomment the line below:\n",
        "ngrok.set_auth_token(\"36eSFPyRmXAfKsTQ7WRyqWpsmDj_25rrefuGQpKA8PJjhAA6e\")\n",
        "\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"ðŸš€ API is live at: {public_url}/chat\")"
      ],
      "metadata": {
        "id": "_XmARUBTd4Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from threading import Thread\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- STEP 1: RELOAD THE MODEL ---\n",
        "print(\"â³ Checking for model in memory...\")\n",
        "\n",
        "# Only load if not already loaded to save time\n",
        "if 'model' not in globals():\n",
        "    print(\"âš™ï¸ Loading Mistral 7B... (This takes about 1-2 minutes)\")\n",
        "\n",
        "    base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    # This is the folder name we used in the fine-tuning step\n",
        "    adapter_path = \"./mistral-7b-measles-qlora-adapter\"\n",
        "\n",
        "    # 4-bit Quantization Config (Critical for Colab T4 GPU)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load Tokenizer & Base Model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load Adapter (if you ran the fine-tuning step previously)\n",
        "    if os.path.exists(adapter_path):\n",
        "        print(f\"âœ… Found fine-tuned adapter at: {adapter_path}. Loading it...\")\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    else:\n",
        "        print(\"âš ï¸ Fine-tuned adapter NOT found. Loading Base Model only.\")\n",
        "        model = base_model\n",
        "\n",
        "    print(\"âœ… Model loaded into memory!\")\n",
        "else:\n",
        "    print(\"âœ… Model is already loaded.\")\n",
        "\n",
        "\n",
        "# --- STEP 2: DEFINE THE API ---\n",
        "app = FastAPI(title=\"Measles Chatbot API\")\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    user_query = request.message\n",
        "\n",
        "    # Format prompt (Instruction Tuned Format)\n",
        "    prompt = f\"### Instruction:\\n{user_query}\\n\\n### Response:\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Parse out the answer\n",
        "    if \"### Response:\" in full_text:\n",
        "        answer = full_text.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_text\n",
        "\n",
        "    return {\"response\": answer}\n",
        "\n",
        "# --- STEP 3: START SERVER & TUNNEL ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
        "\n",
        "# Clean up old tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start server thread\n",
        "thread = Thread(target=run_server)\n",
        "thread.start()\n",
        "\n",
        "# Open Tunnel\n",
        "# Ensure your token is set. If you restarted the runtime, uncomment and paste it here:\n",
        "ngrok.set_auth_token(\"36eSFPyRmXAfKsTQ7WRyqWpsmDj_25rrefuGQpKA8PJjhAA6e\")\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(8000).public_url\n",
        "    print(f\"\\nðŸš€ API IS LIVE AT: {public_url}/chat\")\n",
        "    print(\"You can now send POST requests to this URL from Postman.\")\n",
        "except Exception as e:\n",
        "    print(\"Ngrok Error:\", e)\n",
        "    print(\"Check if your auth token is set correctly.\")"
      ],
      "metadata": {
        "id": "9-1AZEStd7CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. INSTALL DEPENDENCIES (If not already installed) ---\n",
        "!pip install -q -U langchain langchain-community faiss-cpu sentence-transformers pypdf python-docx langchain-text-splitters\n",
        "!pip install -q -U fastapi uvicorn pyngrok nest_asyncio peft transformers bitsandbytes\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import shutil\n",
        "from threading import Thread\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from typing import List\n",
        "\n",
        "# LangChain & RAG Imports\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Model Imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & LOAD RAG VECTOR INDEX\n",
        "# ==========================================\n",
        "print(\"ðŸš€ STARTING SETUP...\")\n",
        "\n",
        "# Define Paths (Aligned with your Drive structure)\n",
        "# Ensure you have run: drive.mount('/content/drive')\n",
        "rag_folder_path = \"/content/drive/MyDrive/Measles_Chatbot_Project/RAG Files\"\n",
        "vector_db_path = \"/content/drive/MyDrive/Measles_Chatbot_Project/faiss_index_measles\"\n",
        "\n",
        "# Initialize Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def build_or_load_vector_db():\n",
        "    # If index already exists, load it\n",
        "    if os.path.exists(vector_db_path):\n",
        "        print(f\"ðŸ“‚ Found existing Vector DB at {vector_db_path}. Loading...\")\n",
        "        try:\n",
        "            return FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error loading index: {e}. Rebuilding...\")\n",
        "\n",
        "    print(\"âš™ï¸ Building new Vector DB from 'RAG Files'...\")\n",
        "    documents = []\n",
        "\n",
        "    # Check if folder exists\n",
        "    if not os.path.exists(rag_folder_path):\n",
        "        os.makedirs(rag_folder_path, exist_ok=True)\n",
        "        print(f\"âš ï¸ Created folder {rag_folder_path}. Please upload your files here!\")\n",
        "        return None\n",
        "\n",
        "    # Load Files\n",
        "    for filename in os.listdir(rag_folder_path):\n",
        "        file_path = os.path.join(rag_folder_path, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            try:\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                documents.extend(loader.load())\n",
        "                print(f\"   - Loaded PDF: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ Error loading PDF {filename}: {e}\")\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            try:\n",
        "                loader = Docx2txtLoader(file_path)\n",
        "                documents.extend(loader.load())\n",
        "                print(f\"   - Loaded DOCX: {filename}\")\n",
        "            except Exception as e:\n",
        "                 print(f\"   âš ï¸ Error loading DOCX {filename}: {e}\")\n",
        "\n",
        "    if not documents:\n",
        "        print(\"âŒ No documents found. Index will be empty.\")\n",
        "        return None\n",
        "\n",
        "    # Split Text\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"ðŸ“Š Created {len(chunks)} chunks.\")\n",
        "\n",
        "    # Create & Save Vector Store\n",
        "    vector_db = FAISS.from_documents(chunks, embedding_model)\n",
        "    vector_db.save_local(vector_db_path)\n",
        "    print(f\"ðŸ’¾ Vector DB saved to {vector_db_path}\")\n",
        "    return vector_db\n",
        "\n",
        "# Initialize Vector DB\n",
        "vector_store = build_or_load_vector_db()\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOAD LLM (MISTRAL)\n",
        "# ==========================================\n",
        "print(\"âš™ï¸ Loading Mistral 7B Model...\")\n",
        "\n",
        "# 1. Config\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 2. Tokenizer & Base Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 3. Load Adapter (Update path if you have trained one)\n",
        "# adapter_path = \"/content/drive/MyDrive/Measles_Chatbot_Project/final_mistral_adapter\"\n",
        "# if os.path.exists(adapter_path):\n",
        "#     print(f\"âœ… Loading Adapter from {adapter_path}\")\n",
        "#     model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "# else:\n",
        "print(\"â„¹ï¸ Using Base Mistral Model (Adapter not found/configured)\")\n",
        "model = base_model\n",
        "\n",
        "print(\"âœ… Model Ready!\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEFINE API (FastAPI)\n",
        "# ==========================================\n",
        "app = FastAPI(title=\"ZNPHI Measles Chatbot\")\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    user_query = request.message\n",
        "\n",
        "    # 1. Retrieve Context\n",
        "    context_text = \"\"\n",
        "    source_docs = []\n",
        "\n",
        "    if vector_store:\n",
        "        results = vector_store.similarity_search(user_query, k=3)\n",
        "        for doc in results:\n",
        "            context_text += f\"\\n- {doc.page_content}\"\n",
        "            source_docs.append(doc.metadata.get(\"source\", \"unknown\"))\n",
        "    else:\n",
        "        context_text = \"No context available.\"\n",
        "\n",
        "    # 2. Format Prompt\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are an expert medical assistant for measles surveillance in Zambia.\n",
        "Use the Context below to answer the Question strictly based on the guidelines.\n",
        "If the answer is not in the context, say \"I cannot find that information in the guidelines.\"\n",
        "\n",
        "### Context:{context_text}\n",
        "\n",
        "### Question:\n",
        "{user_query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # 3. Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.3,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"### Response:\" in full_response:\n",
        "        answer = full_response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_response\n",
        "\n",
        "    return {\n",
        "        \"response\": answer,\n",
        "        \"sources\": list(set(source_docs))\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 4. START SERVER (Ngrok + Uvicorn)\n",
        "# ==========================================\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_server():\n",
        "    # Force IPv4 to avoid \"Connection Refused\"\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
        "\n",
        "# Kill old tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Server Thread\n",
        "thread = Thread(target=run_server)\n",
        "thread.start()\n",
        "\n",
        "# Set Auth Token (Uncomment and add your token if needed)\n",
        "ngrok.set_auth_token(\"36eSFPyRmXAfKsTQ7WRyqWpsmDj_25rrefuGQpKA8PJjhAA6e\") # <--- REPLACE WITH YOUR NGROK AUTH TOKEN\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(8000).public_url\n",
        "    print(f\"\\nðŸš€ API IS LIVE AT: {public_url}/chat\")\n",
        "    print(\"Use Postman with method: POST\")\n",
        "except Exception as e:\n",
        "    print(\"Ngrok Error:\", e)"
      ],
      "metadata": {
        "id": "_vbhiPhxd-Qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993989ab-b0b4-4079-ecdd-c2d5755f9517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/102.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.20.0 requires fastapi<0.119.0,>=0.115.0, but you have fastapi 0.124.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}